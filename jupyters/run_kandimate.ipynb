{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a11eb9-47fc-41d8-b663-ae2ffe35a158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder ./models/kandinsky-2-2-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531accca-c425-4947-8e5e-024acdbbc0e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/kandinsky-community/kandinsky-2-2-prior ./models/kandinsky-2-2-prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38246200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download models from https://drive.google.com/drive/folders/1GYMJ6ZJMljikSPkbJQNIbORqtdJjHBD0?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9a35a1-5e74-44ee-aff8-c292e45297d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import datetime\n",
    "import inspect\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer\n",
    "from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline\n",
    "from diffusers import DDIMScheduler, DDPMScheduler, VQModel\n",
    "from diffusers.pipelines.kandinsky import MultilingualCLIP\n",
    "\n",
    "from kandimate.models.unet import UNet3DConditionModel\n",
    "from kandimate.pipelines.pipeline_kandimation import KandimatePipeline\n",
    "from kandimate.utils.util import save_videos_grid\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3719d8ce-7fcf-4488-9edb-5e7753dc592a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_gif_in_jupyter(gif_path, width=380):\n",
    "    from IPython import display\n",
    "    return display.HTML(f'<img src=\"{gif_path}\" width=\"{width}\">')\n",
    "\n",
    "def register_module_weights(pipe, module_weights):\n",
    "    for up_block in pipe.unet.up_blocks: \n",
    "        block_name = up_block.__class__.__name__\n",
    "        if block_name in module_weights:\n",
    "            up_block.module_weights = module_weights[block_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10002433",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "prior_path = '../../models/kandinsky-2-2-prior'\n",
    "decoder_path = '../../models/kandinsky-2-2-decoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0204b442-4f4a-41af-82e3-7c5deb68ffd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(prior_path, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "    prior_path, \n",
    "    subfolder='text_encoder',\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d3e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = DDPMScheduler.from_pretrained(decoder_path, subfolder='scheduler')\n",
    "# scheduler = DDIMScheduler(**{\n",
    "#     'beta_start': 0.00085,\n",
    "#     'beta_end': 0.012,\n",
    "#     'beta_schedule': \"linear\",\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31805c30-36d6-4c1b-bbdc-91a435dd4dda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movq = VQModel.from_pretrained(\n",
    "    decoder_path, \n",
    "    subfolder=\"movq\", \n",
    "    torch_dtype=torch.float16,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2697b100-408a-454b-aae9-0af7cd00c7e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original unet's pretrained weights from ../../models/kandinsky-2-2-decoder/unet ...\n",
      "### missing keys: 1512; \n",
      "### unexpected keys: 0;\n",
      "### Motion Module Parameters: 321.823488 M\n"
     ]
    }
   ],
   "source": [
    "unet_additional_kwargs = {\n",
    "    \"use_motion_module\": True,\n",
    "    \"motion_module_resolutions\": [1, 2, 4, 8],\n",
    "    \"motion_module_mid_block\": False,\n",
    "    \"motion_module_decoder_only\": False,\n",
    "    \"motion_module_kwargs\": {\n",
    "        \"num_layers\": 2,\n",
    "        \"num_attention_heads\": 8,\n",
    "        \"temporal_position_encoding\": True,\n",
    "        \"temporal_position_encoding_max_len\": 24,\n",
    "    },\n",
    "}\n",
    "\n",
    "unet = UNet3DConditionModel.from_pretrained_2d(\n",
    "    decoder_path, \n",
    "    subfolder=\"unet\",\n",
    "    unet_additional_kwargs=unet_additional_kwargs,\n",
    ")\n",
    "unet = unet.to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85cd747b-e326-4cb8-afb1-14178624925a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(724, 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_module = '../../models/motion-modules/checkpoint-65000.ckpt'\n",
    "motion_module_state_dict = torch.load(motion_module, map_location=\"cpu\")\n",
    "\n",
    "state_dict = {}\n",
    "for name, tensor in motion_module_state_dict['state_dict'].items():\n",
    "    state_dict[name.replace('module.', '')] = tensor\n",
    "    \n",
    "missing, unexpected = unet.load_state_dict(state_dict, strict=False)\n",
    "assert len(unexpected) == 0\n",
    "len(missing), len(unexpected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "990a83e2-d0ae-4519-ba73-7ccaa636207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = KandimatePipeline(\n",
    "    movq=movq, \n",
    "    text_encoder=text_encoder, \n",
    "    tokenizer=tokenizer, \n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e372bacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d289d950b6478fa505db9f74e1a73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe_prior = KandinskyV22PriorPipeline.from_pretrained(\n",
    "    prior_path, torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5625451",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_weights = {\n",
    "    'SimpleCrossAttnUpBlock3D': {\n",
    "        1536: {\n",
    "            'backbone_coef': 1.4,\n",
    "            'skip_coef': 0.9,\n",
    "        },\n",
    "        1152: {\n",
    "            'backbone_coef': 1.2,\n",
    "            'skip_coef': 0.2,\n",
    "        },\n",
    "        768: {\n",
    "            'backbone_coef': 1.1,\n",
    "            'skip_coef': 0.05,\n",
    "        },\n",
    "    },\n",
    "    'ResnetUpsampleBlock3D': {},\n",
    "#     'SimpleCrossAttnUpBlock3D': {},\n",
    "}\n",
    "\n",
    "register_module_weights(pipeline, module_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30ae916b-e4a1-40a7-ac93-03a0aaf0e247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a63f76cf6604cc3a71a335cfb1ed939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6087aba51b0e48e5a44b5477cac27a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IMG_H = 768\n",
    "IMG_W = 768\n",
    "TOTAL_FRAMES = 16\n",
    "\n",
    "guidance_scale = 5\n",
    "num_inference_steps = 25\n",
    "\n",
    "gif_count = 1\n",
    "# seeds = [np.random.randint(0, np.iinfo(np.intp).max) for _ in range(gif_count)]\n",
    "seeds = [17]\n",
    "\n",
    "prompts = [\n",
    "#     'pretty anime girl looking at the camera, cinematic, 4k, 8k',\n",
    "#     'pretty girl looking at the camera, extremely high detail, 8k, 4k, cinematic',\n",
    "    'pretty redhead girl looking at the camera, cinematic, extremely high detail, 8k, 4k, HQ',\n",
    "#     'rzd train is going, cinematic, 4k, 8k',\n",
    "#     'iron man is landing, avengers, cinematic, 4k, 8k',\n",
    "#     'a dog, cinematic, 8k, 4k, HQ',\n",
    "]\n",
    "n_prompts = {\n",
    "#     'no': '',\n",
    "#     'short': 'low quality, bad quality',\n",
    "    'long': 'lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers',\n",
    "}\n",
    "\n",
    "for seed in seeds:\n",
    "    samples = []\n",
    "    for neg_name, negative_prompt in n_prompts.items():\n",
    "        for prompt in prompts:\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "            image_emb, zero_image_emb = pipe_prior(\n",
    "                prompt=prompt, negative_prompt=negative_prompt, generator=generator, \n",
    "            ).to_tuple()\n",
    "\n",
    "            sample = pipeline(\n",
    "                prompt,\n",
    "                image_embeds = image_emb.to(dtype=torch.float16),\n",
    "                negative_image_embeds = zero_image_emb.to(dtype=torch.float16),\n",
    "                negative_prompt = negative_prompt,\n",
    "                num_inference_steps = num_inference_steps,\n",
    "                guidance_scale = guidance_scale,\n",
    "                width = IMG_W,\n",
    "                height = IMG_H,\n",
    "                video_length = TOTAL_FRAMES,\n",
    "                generator = generator,\n",
    "                use_progress_bar = True,\n",
    "            ).videos\n",
    "\n",
    "            samples.append(sample)\n",
    "            savedir = f'../samples/generation/{seed}_{neg_name}'\n",
    "            prompt = \"-\".join((prompt.replace(\"/\", \"\").split(\" \")[:10]))\n",
    "            gif_path = f\"{savedir}/{prompt}.gif\"\n",
    "            save_videos_grid(sample, gif_path)\n",
    "\n",
    "    savedir = f'../samples/generation'\n",
    "    gif_path = f\"{savedir}/{seed}.gif\"\n",
    "    save_videos_grid(torch.cat(samples), gif_path, n_rows=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49415278-9019-4348-83be-4d47e8c112a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2704cc2-7e1f-4074-994e-f6e371ab8c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfde407-f173-442f-9785-c99f5d51f98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968df00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
