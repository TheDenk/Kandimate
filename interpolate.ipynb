{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bff1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "from PIL import Image  \n",
    "from denku import show_images\n",
    "\n",
    "from kandimate.utils.util import save_videos_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba5b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "precision = torch.float16\n",
    "model_path = '../models/interpolation-models/film_net_fp16.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c931282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(model_path, map_location='cpu')\n",
    "model.eval().to(device=device, dtype=precision);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be181d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = './samples/example.gif'\n",
    "\n",
    "gif_file = imageio.get_reader(gif_path)\n",
    "images = [np.array(x) for x in gif_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92529f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_list_to_torch(images):\n",
    "    tensor = [torch.from_numpy(x) for x in images]\n",
    "    tensor = [x.permute(2, 0, 1).unsqueeze(0) for x in tensor]\n",
    "    tensor = [x.float() / 255.0 for x in tensor]\n",
    "    tensor = torch.cat(tensor)\n",
    "    return tensor\n",
    "\n",
    "def torch_to_np(tensor):\n",
    "    images = tensor.permute(0, 2, 3, 1).numpy()\n",
    "    images = np.clip(images * 255, 0, 255)\n",
    "    images = images.astype(np.uint8)\n",
    "    return images\n",
    "\n",
    "tensor = pil_list_to_torch(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee5805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_frames = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6427eda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 3, 768, 768]), torch.Size([15, 3, 768, 768]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_tensor = tensor[0:-1:1].to(precision).to(device)\n",
    "next_tensor = tensor[1::1].to(precision).to(device)\n",
    "dt = prev_tensor.new_full((1, 1), 0.5)\n",
    "\n",
    "prev_tensor.shape, next_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52b838a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../third_party/nvfuser/csrc/parser.cpp:3777.)\n",
      "  return forward_call(*args, **kwargs)\n",
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1003.)\n",
      "  return forward_call(*args, **kwargs)\n",
      "/home/user/Projects/denk_baseline/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501: UserWarning: operator() profile_node %511 : int[] = prim::profile_ivalue[profile_failed=\"varying profile values\"](%509)\n",
      " does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dt_images_mid = model(prev_tensor, next_tensor, dt).cpu()\n",
    "    generated_prev = torch_to_np(dt_images_mid)\n",
    "    \n",
    "if interpolation_frames > 1:\n",
    "    with torch.no_grad():\n",
    "        dt_images_prev = model(prev_tensor.to(device), dt_images_mid.to(device), dt).cpu()\n",
    "        dt_images_next = model(dt_images_mid.to(device), next_tensor.to(device), dt).cpu()\n",
    "        \n",
    "    generated_mid = torch_to_np(dt_images_mid)\n",
    "    generated_next = torch_to_np(dt_images_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a88daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_images = [images[0]]\n",
    "\n",
    "if interpolation_frames == 1:\n",
    "    for mid_img, orig_img in zip(generated_mid, images[1:]):\n",
    "        extended_images.extend([mid_img, orig_img])\n",
    "elif interpolation_frames == 2:\n",
    "    for prev_img, next_img, orig_img in zip(generated_prev, generated_next, images[1:]):\n",
    "        extended_images.extend([prev_img, next_img, orig_img])\n",
    "elif interpolation_frames == 3:\n",
    "    for prev_img, mid_img, next_img, orig_img in zip(generated_prev, generated_mid, generated_next, images[1:]):\n",
    "        extended_images.extend([prev_img, mid_img, next_img, orig_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69dd7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('./samples/test.gif', extended_images, duration=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f009e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_gif(in_path, out_path, img_h=512, img_w=512, duration=60):\n",
    "    gif_file = imageio.get_reader(in_path)\n",
    "    images = [cv2.resize(x, (img_w, img_h)) for x in gif_file]\n",
    "    imageio.mimsave(out_path, images, duration=duration)\n",
    "    \n",
    "resize_gif(\n",
    "    in_path='./samples/test.gif', \n",
    "    out_path='./samples/resized.gif',\n",
    "    img_h=384, \n",
    "    img_w=384,\n",
    "    duration=90,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789c952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a745468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a3497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52981a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ff202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a39610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
